import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import LeaveOneOut
from tqdm import tqdm
import seaborn as sns
import os
import time

# =====================================================
# 1. DOWNLOAD & LOAD DATASET
# =====================================================
print("="*60)
print("EMNIST LETTERS - HOG + SVM + LOOCV")
print("="*60)

path = kagglehub.dataset_download("crawford/emnist")
print(f"Dataset path: {path}")

# Load data
train_file = os.path.join(path, 'emnist-letters-train.csv')
test_file = os.path.join(path, 'emnist-letters-test.csv')

print(f"\nLoading training data...")
train_data = pd.read_csv(train_file)
print(f"Loading testing data...")
test_data = pd.read_csv(test_file)

print(f"\nTrain: {train_data.shape}, Test: {test_data.shape}")

# Pisahkan label dan features
y_train = train_data.iloc[:, 0].values
X_train = train_data.iloc[:, 1:].values
y_test = test_data.iloc[:, 0].values
X_test = test_data.iloc[:, 1:].values

print(f"Training samples: {len(X_train)}, Test samples: {len(X_test)}")
print(f"Classes: {len(np.unique(y_train))} (Labels: {np.min(y_train)}-{np.max(y_train)})")

# =====================================================
# 2. PREPROCESSING & HOG EXTRACTION
# =====================================================
def preprocess_emnist_image(image_flat):
    """Rotate dan flip EMNIST image"""
    image = image_flat.reshape(28, 28)
    image = np.rot90(image)
    image = np.fliplr(image)
    return image

def extract_hog_features(images, dataset_name=""):
    """Ekstraksi HOG features"""
    hog_features = []
    print(f"\nEkstraksi HOG untuk {dataset_name}...")
    
    for i, image_flat in enumerate(images):
        if (i + 1) % 10000 == 0:
            print(f"  Progress: {i + 1}/{len(images)} ({(i + 1)/len(images)*100:.1f}%)")
        
        image = preprocess_emnist_image(image_flat)
        feature = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
        hog_features.append(feature)
    
    print(f"  Selesai! Shape: {np.array(hog_features).shape}")
    return np.array(hog_features)

print("\n" + "="*60)
print("EKSTRAKSI FITUR HOG")
print("="*60)

X_train_hog = extract_hog_features(X_train, "training data")
X_test_hog = extract_hog_features(X_test, "testing data")

print(f"\nHOG feature dimension: {X_train_hog.shape[1]}")

# =====================================================
# 3. LOOCV DENGAN CHECKPOINT
# =====================================================
print("\n" + "="*60)
print("LEAVE-ONE-OUT CROSS-VALIDATION")
print("="*60)

# Konfigurasi
USE_LOOCV_SUBSET = True     # Set False untuk full dataset
LOOCV_SUBSET_SIZE = 1000    # Ukuran subset
BATCH_SIZE = 100             # Save checkpoint setiap 100 iterasi
SAVE_FILE = "loocv_results_emnist_letters.csv"

# Pilih data untuk LOOCV
if USE_LOOCV_SUBSET:
    print(f"\n⚠️  Mode: SUBSET ({LOOCV_SUBSET_SIZE} samples)")
    print(f"    Set USE_LOOCV_SUBSET=False untuk full dataset\n")
    
    from sklearn.model_selection import train_test_split
    X_loocv, _, y_loocv, _ = train_test_split(
        X_train_hog, y_train, 
        train_size=LOOCV_SUBSET_SIZE, 
        stratify=y_train,
        random_state=42
    )
else:
    print(f"\n⚠️  Mode: FULL DATASET ({len(X_train_hog)} samples)")
    print(f"    Estimasi waktu: {len(X_train_hog) * 2 / 3600:.1f} jam\n")
    X_loocv = X_train_hog
    y_loocv = y_train

n_samples = X_loocv.shape[0]
print(f"LOOCV samples: {n_samples}")
print(f"Checkpoint file: {SAVE_FILE}")

# Buat model SVM
svm_clf = make_pipeline(
    StandardScaler(), 
    SVC(kernel='rbf', C=10, gamma=0.01)
)

# Cek checkpoint
try:
    df_checkpoint = pd.read_csv(SAVE_FILE)
    start_index = len(df_checkpoint)
    y_true_all = df_checkpoint["y_true"].tolist()
    y_pred_all = df_checkpoint["y_pred"].tolist()
    print(f"\n✓ Resume from index {start_index}/{n_samples} ({start_index/n_samples*100:.1f}%)")
except FileNotFoundError:
    start_index = 0
    y_true_all = []
    y_pred_all = []
    print(f"\n✓ Starting from scratch")

# Inisialisasi LOOCV
cv = LeaveOneOut()
splits = list(cv.split(X_loocv))

print(f"\nMemulai LOOCV dari iterasi {start_index}...")
print(f"Checkpoint: setiap {BATCH_SIZE} iterasi\n")

start_time = time.time()

# LOOCV Loop dengan progress bar
for i in tqdm(range(start_index, len(splits)), 
              total=len(splits), 
              initial=start_index, 
              desc="LOOCV",
              unit="sample"):
    
    train_ix, test_ix = splits[i]
    X_train_fold = X_loocv[train_ix]
    X_test_fold = X_loocv[test_ix]
    y_train_fold = y_loocv[train_ix]
    y_test_fold = y_loocv[test_ix]
    
    # Train & predict
    svm_clf.fit(X_train_fold, y_train_fold)
    y_pred = svm_clf.predict(X_test_fold)
    
    y_true_all.append(y_test_fold[0])
    y_pred_all.append(y_pred[0])
    
    # Checkpoint
    if (i + 1) % BATCH_SIZE == 0:
        pd.DataFrame({"y_true": y_true_all, "y_pred": y_pred_all}).to_csv(SAVE_FILE, index=False)

# Save final
pd.DataFrame({"y_true": y_true_all, "y_pred": y_pred_all}).to_csv(SAVE_FILE, index=False)

total_time = time.time() - start_time
print(f"\n✓ LOOCV selesai!")
print(f"  Waktu: {total_time/60:.2f} menit")
print(f"  Avg: {total_time/(n_samples-start_index):.3f} detik/sample")

# Hasil LOOCV
y_true = np.array(y_true_all)
y_pred = np.array(y_pred_all)
loocv_accuracy = accuracy_score(y_true, y_pred)

print(f"\n{'='*60}")
print(f"HASIL LOOCV:")
print(f"  Samples: {n_samples}")
print(f"  Akurasi: {loocv_accuracy * 100:.2f}%")
print(f"{'='*60}")

# Classification Report
def label_to_letter(label):
    return chr(64 + int(label))

target_names = [label_to_letter(i) for i in sorted(np.unique(y_loocv))]
print("\nClassification Report (LOOCV):")
print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))

# Confusion Matrix
cm_loocv = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(14, 12))
labels = [label_to_letter(i) for i in sorted(np.unique(y_loocv))]
sns.heatmap(cm_loocv, annot=True, fmt='d', cmap='Greens', 
            xticklabels=labels, yticklabels=labels, cbar=True)
plt.title(f'Confusion Matrix - LOOCV (n={n_samples})', fontsize=14)
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# =====================================================
# 4. EVALUASI TEST SET
# =====================================================
print("\n" + "="*60)
print("EVALUASI TEST SET")
print("="*60)

print(f"\nTraining final model dengan semua data ({len(X_train_hog)} samples)...")
model_final = make_pipeline(
    StandardScaler(), 
    SVC(kernel='rbf', C=10, gamma=0.01)
)
model_final.fit(X_train_hog, y_train)
print("✓ Training selesai!")

# Prediksi test set
print("\nPrediksi test set...")
y_pred_test = model_final.predict(X_test_hog)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"\n{'='*60}")
print(f"AKURASI TEST SET: {test_accuracy * 100:.2f}%")
print(f"{'='*60}")

# Classification Report Test Set
target_names_test = [label_to_letter(i) for i in sorted(np.unique(y_train))]
print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_pred_test, target_names=target_names_test, zero_division=0))

# Confusion Matrix Test Set
cm_test = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(14, 12))
labels_test = [label_to_letter(i) for i in sorted(np.unique(y_train))]
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', 
            xticklabels=labels_test, yticklabels=labels_test, cbar=True)
plt.title('Confusion Matrix - Test Set', fontsize=14)
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# =====================================================
# 5. RINGKASAN
# =====================================================
print("\n" + "="*60)
print("RINGKASAN HASIL")
print("="*60)

print(f"\nDataset:")
print(f"  • Training: {len(X_train)} samples")
print(f"  • Testing: {len(X_test)} samples")
print(f"  • Classes: {len(np.unique(y_train))}")
print(f"  • HOG features: {X_train_hog.shape[1]} dimensions")

print(f"\nModel:")
print(f"  • Algorithm: SVM (RBF kernel)")
print(f"  • Parameters: C=10, gamma=0.01")

print(f"\nLOOCV Results:")
print(f"  • Samples: {n_samples}")
print(f"  • Akurasi: {loocv_accuracy * 100:.2f}%")
print(f"  • Waktu: {total_time/60:.2f} menit")

print(f"\nTest Set Results:")
print(f"  • Akurasi: {test_accuracy * 100:.2f}%")

print(f"\n{'='*60}")
if USE_LOOCV_SUBSET:
    print(f"⚠️  LOOCV dilakukan pada subset {LOOCV_SUBSET_SIZE} samples")
    print(f"    Set USE_LOOCV_SUBSET=False untuk full dataset")
else:
    print(f"✓  LOOCV dilakukan pada SEMUA data training")
print(f"{'='*60}")

print("\n✓ Program selesai!")
